# Attention Layer Visualizations

This document explains the three graphs generated by `visualize_attention.py` that show how the TemporalSelfAttention layer works with time series data.

## Overview

The visualizations demonstrate how the attention mechanism processes temporal sequences by showing:
1. **Attention patterns** - which time steps attend to which others
2. **Embedding changes** - how representations are modified by attention
3. **Attention focus** - where different time steps "look" for information

## Graph 1: Attention Patterns (Heatmap)

**What it shows:** A 20Ã—20 heatmap where each cell (i,j) represents how much time step i attends to time step j.

**How to read it:**
- **Rows (Y-axis):** Query positions (the time step asking "what should I attend to?")
- **Columns (X-axis):** Key positions (the time steps being attended to)
- **Color intensity:** Attention weight (darker blue = stronger attention)
- **Diagonal:** Self-attention (how much each time step attends to itself)

**Key insights:**
- **Diagonal patterns:** Strong diagonal indicates the model focuses on nearby time steps
- **Off-diagonal patterns:** Show long-range temporal dependencies
- **Row patterns:** Each row shows the attention distribution for one query position
- **Symmetry:** Asymmetric patterns indicate directional temporal relationships

**Example interpretation:**
- If row 10 has high values in columns 8-12, it means time step 10 pays attention to time steps 8-12
- Bright spots off the diagonal indicate important temporal relationships

## Graph 2: Embedding Changes (Before vs After Attention)

**What it shows:** Comparison of embedding magnitudes before and after applying attention.

**How to read it:**
- **Blue line (circles):** L2 norm of embeddings before attention
- **Orange line (squares):** L2 norm of embeddings after attention
- **X-axis:** Time steps (0 to 19)
- **Y-axis:** Embedding magnitude (L2 norm)

**Key insights:**
- **Magnitude changes:** Shows how attention modifies the strength of representations
- **Smoothing effect:** Attention often smooths out extreme values
- **Enhancement:** Some time steps may have increased magnitude after attention
- **Temporal patterns:** Changes reveal which time steps are most affected by attention

**Example interpretation:**
- If the orange line is consistently lower, attention is regularizing the embeddings
- Peaks in the orange line indicate time steps that received strong attention from others
- Large differences between lines show where attention had the most impact

## Graph 3: Attention Focus (Multiple Query Positions)

**What it shows:** Attention patterns from 5 different query positions (t=0, 5, 10, 15, 19).

**How to read it:**
- **Different colored lines:** Each represents a different query time step
- **X-axis:** Key positions (what they're attending to)
- **Y-axis:** Attention weight
- **Legend:** Shows which query position each line represents

**Key insights:**
- **Local vs global attention:** Narrow peaks indicate local focus, broad distributions indicate global attention
- **Positional bias:** Shows if certain positions (beginning, middle, end) have different attention patterns
- **Temporal locality:** Peaks near the query position indicate local temporal dependencies
- **Long-range dependencies:** Peaks far from the query position show long-range relationships

**Example interpretation:**
- If "Query at t=0" has a peak at position 0, it shows strong self-attention
- If "Query at t=10" has peaks at positions 8-12, it shows local temporal attention
- If any query has peaks at distant positions, it indicates long-range dependencies

## Technical Details

**Data:** 20 time steps from the HUFL variable of the ETT dataset
**Model:** TemporalSelfAttention with 4 heads, 32-dimensional embeddings
**Attention weights:** Averaged across all 4 attention heads for clarity
**Normalization:** Attention weights sum to approximately 1 for each query position

## Usage

To generate these visualizations:

```bash
python visualize_attention.py
```

The script will:
1. Load sample time series data
2. Process it through the UnivariateFunctionLearner
3. Apply TemporalSelfAttention
4. Generate and save the three visualization graphs
5. Print key insights about the attention patterns

## Files Generated

- `attention_visualizations.png`: The three graphs described above
- Console output with numerical insights about attention patterns